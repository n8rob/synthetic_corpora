# -*- coding: utf-8 -*-
"""Copy of Haitian_Translation_Transformer_Full_Bitext.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f_FHQ3V-VI6znC4WZ1aFUT6uSY3GoBzj

<a
href="https://colab.research.google.com/github/wingated/cs474_labs_f2019/blob/master/DL_Lab7.ipynb"
  target="_parent">
  <img
    src="https://colab.research.google.com/assets/colab-badge.svg"
    alt="Open In Colab"/>
</a>

## Setup
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import math, copy, time
from torch.autograd import Variable
import matplotlib.pyplot as plt
import seaborn
seaborn.set_context(context="talk")
import argparse
import sacrebleu
from sacremoses import MosesDetokenizer
md = MosesDetokenizer(lang='en')
import pdb
import pickle as pkl
# %matplotlib inline

"""#Model

## Model Helpers
"""

class PositionwiseFeedForward(nn.Module):
    "Simple linear layers with dropout and relu"
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.w_1 = nn.Linear(d_model, d_ff)
        self.w_2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return self.w_2(self.dropout(F.relu(self.w_1(x))))

class Embeddings(nn.Module):
    "Create word embeddings"
    def __init__(self, d_model, vocab):
        super(Embeddings, self).__init__()
        self.lut = nn.Embedding(vocab, d_model)
        self.d_model = d_model

    def forward(self, x):
        return self.lut(x) * math.sqrt(self.d_model)

class Generator(nn.Module):
    "Define standard linear + softmax generation step."
    def __init__(self, d_model, vocab):
        super(Generator, self).__init__()
        self.proj = nn.Linear(d_model, vocab)

    def forward(self, x):
        return F.log_softmax(self.proj(x), dim=-1)

class LayerNorm(nn.Module):
    "Construct a layernorm module "
    def __init__(self, features, eps=1e-6):
        super(LayerNorm, self).__init__()
        self.a_2 = nn.Parameter(torch.ones(features))
        self.b_2 = nn.Parameter(torch.zeros(features))
        self.eps = eps

    def forward(self, x):
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True)
        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2

class SublayerConnection(nn.Module):
    """
    A residual connection followed by a layer norm.
    Note for code simplicity the norm is first as opposed to last.
    """
    def __init__(self, size, dropout):
        super(SublayerConnection, self).__init__()
        self.norm = LayerNorm(size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, sublayer):
        "Apply residual connection to any sublayer with the same size."
        return x + self.dropout(sublayer(self.norm(x)))

def clones(module, N):
    "Produce N identical layers."
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])

"""## Encoder

The encoder is composed of a stack of $N=6$ identical layers.
"""

class Encoder(nn.Module):
    "Core encoder is a stack of N layers"
    def __init__(self, layer, N):
        super(Encoder, self).__init__()
        self.layers = clones(layer, N)
        self.norm = LayerNorm(layer.size)

    def forward(self, x, mask):
        "Pass the input (and mask) through each layer in turn."
        for layer in self.layers:
            x = layer(x, mask)
        return self.norm(x)



class EncoderLayer(nn.Module):
    "Encoder is made up of self-attn and feed forward "
    def __init__(self, size, self_attn, feed_forward, dropout):
        super(EncoderLayer, self).__init__()
        self.self_attn = self_attn
        self.feed_forward = feed_forward
        self.sublayer = clones(SublayerConnection(size, dropout), 2)
        self.size = size

    def forward(self, x, mask):
        "Follow Figure 1 (left) for connections."
        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
        return self.sublayer[1](x, self.feed_forward)

"""## Decoder

The decoder is also composed of a stack of $N=6$ identical layers.

"""

class Decoder(nn.Module):
    "Generic N layer decoder with masking."
    def __init__(self, layer, N):
        super(Decoder, self).__init__()
        self.layers = clones(layer, N)
        self.norm = LayerNorm(layer.size)

    def forward(self, x, memory, src_mask, tgt_mask):
        for layer in self.layers:
            x = layer(x, memory, src_mask, tgt_mask)
        return self.norm(x)

class DecoderLayer(nn.Module):
    "Decoder is made of self-attn, src-attn, and feed forward (defined below)"
    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):
        super(DecoderLayer, self).__init__()
        self.size = size
        self.self_attn = self_attn
        self.src_attn = src_attn
        self.feed_forward = feed_forward
        self.sublayer = clones(SublayerConnection(size, dropout), 3)

    def forward(self, x, memory, src_mask, tgt_mask):
        "Follow Figure 1 (right) for connections."
        m = memory
        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))
        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))
        return self.sublayer[2](x, self.feed_forward)

def subsequent_mask(size):
    "Mask out subsequent positions."
    attn_shape = (1, size, size)
    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')
    return torch.from_numpy(subsequent_mask) == 0

"""##Implement Attention


https://arxiv.org/pdf/1706.03762.pdf

"""

def attention(query, key, value, mask=None):
    # Compute 'Scaled Dot Product Attention'
    # print("Q")
    # print(query.size())
    # print("K")
    # print(key.size())
    # print("V")
    # print(value.size())

    # First we want to find out the scaling denominator and the softmax
    d_k = query.size()[2]
    scale_denom = np.sqrt(d_k)
    softmax = nn.Softmax(dim=-1)

    # scores = QK^T/scale
    # Matrix multiplication
    QK = torch.bmm(query, torch.transpose(key, 1, 2))
    # Scale
    scores = QK/scale_denom

    # Apply the mask
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)

    # output = softmax(scores)(V)
    # Softmax
    scores = softmax(scores)
    # Matrix multipication with value
    output = torch.bmm(scores, value)

    return output

class MultiHeadedAttention(nn.Module):
    def __init__(self, h, d_model, dropout=0.1):
        super(MultiHeadedAttention, self).__init__()
        # Implement Multi-head attention mechanism

        # Make an attention head (linear layers for q, k, and v)
        # Make h copies of the attention head (Hint: See the `clone()` helper function)
        self.lin_v = clones(nn.Linear(d_model, d_model//h), h)
        self.lin_k = clones(nn.Linear(d_model, d_model//h), h)
        self.lin_q = clones(nn.Linear(d_model, d_model//h), h)
        self.h = h
        self.lin_final = nn.Linear(d_model, d_model)

    def forward(self, query, key, value, mask):
        # For each attention head
            # Pass the query, key, value through their respective layers
            # Compute scaled dot-product attention on the output

        scaled = []
        for i in range(self.h):
            scaled_i = attention( self.lin_q[i](query), self.lin_k[i](key), self.lin_v[i](value), mask )
            scaled.append( scaled_i )
        concatenated = scaled[0]
        for scaled_i in scaled[1:]:
            concatenated = torch.cat((concatenated, scaled_i), dim=-1)

        return self.lin_final(concatenated)

"""## Positional Encoding
Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.  To this end, we add "positional encodings" to the input embeddings at the bottoms of the encoder and decoder stacks.  The positional encodings have the same dimension $d_{\text{model}}$ as the embeddings, so that the two can be summed.   There are many choices of positional encodings, learned and fixed [(cite)](https://arxiv.org/pdf/1705.03122.pdf).

In this work, we use sine and cosine functions of different frequencies:
$$PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\text{model}}})$$

$$PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\text{model}}})$$
where $pos$ is the position and $i$ is the dimension.  That is, each dimension of the positional encoding corresponds to a sinusoid.  The wavelengths form a geometric progression from $2\pi$ to $10000 \cdot 2\pi$.  We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$.

In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.  For the base model, we use a rate of $P_{drop}=0.1$.
"""

class PositionalEncoding(nn.Module):
    "Implement the PE function."
    def __init__(self, d_model, dropout, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        # Compute the positional encodings once in log space.
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        div_term = 1 / (10000 ** (torch.arange(0., d_model, 2) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + Variable(self.pe[:, :x.size(1)],
                         requires_grad=False)
        return self.dropout(x)

# plt.figure(figsize=(15, 5))
# pe = PositionalEncoding(20, 0)
# y = pe.forward(Variable(torch.zeros(1, 100, 20)))
# plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())
# plt.legend(["dim %d"%p for p in [4,5,6,7]])
# None

"""## Full Model"""

class TransformerModel(nn.Module):
    """
    Full transformer model
    """
    def __init__(self, src_vocab, tgt_vocab, N=6, d_model=256, d_ff=1024, h=8, dropout=0.1):
        super(TransformerModel, self).__init__()

        attn = MultiHeadedAttention(h, d_model)
        ff = PositionwiseFeedForward(d_model, d_ff, dropout)
        position = PositionalEncoding(d_model, dropout)
        c = copy.deepcopy

        self.encoder = Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N)
        self.decoder = Decoder(DecoderLayer(d_model, c(attn), c(attn),
                             c(ff), dropout), N)
        self.src_embed = nn.Sequential(Embeddings(d_model, src_vocab), c(position))
        self.tgt_embed = nn.Sequential(Embeddings(d_model, tgt_vocab), c(position))
        self.generator = Generator(d_model, tgt_vocab)
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)

    def forward(self, src, tgt, src_mask, tgt_mask):
        "Take in and process masked src and target sequences."
        return self.decode(self.encode(src, src_mask), src_mask,
                            tgt, tgt_mask)

    def encode(self, src, src_mask):
        return self.encoder(self.src_embed(src), src_mask)

    def decode(self, memory, src_mask, tgt, tgt_mask):
        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)

"""# Training

## Batches and Masking
"""

class Batch:
    "Object for holding a batch of data with mask during training."
    def __init__(self, src, trg=None, pad=0):
        self.src = src
        self.src_mask = (src != pad).unsqueeze(-2)
        if trg is not None:
            self.trg = trg[:, :-1]
            self.trg_y = trg[:, 1:]
            self.trg_mask = \
                self.make_std_mask(self.trg, pad)
            self.ntokens = (self.trg_y != pad).data.sum()

    @staticmethod
    def make_std_mask(tgt, pad):
        "Create a mask to hide padding and future words."
        tgt_mask = (tgt != pad).unsqueeze(-2)
        tgt_mask = tgt_mask & Variable(
            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))
        return tgt_mask


global max_src_in_batch, max_tgt_in_batch
def batch_size_fn(new, count, sofar):
    "Keep augmenting batch and calculate total number of tokens + padding."
    global max_src_in_batch, max_tgt_in_batch
    if count == 1:
        max_src_in_batch = 0
        max_tgt_in_batch = 0
    max_src_in_batch = max(max_src_in_batch,  len(new.src))
    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)
    src_elements = count * max_src_in_batch
    tgt_elements = count * max_tgt_in_batch
    return max(src_elements, tgt_elements)

"""## Label Smoothing

During training, we employed label smoothing of value $\epsilon_{ls}=0.1$ [(cite)](https://arxiv.org/abs/1512.00567).  This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.
"""

class LabelSmoothing(nn.Module):
    "Implement label smoothing."
    def __init__(self, size, padding_idx, smoothing=0.0):
        super(LabelSmoothing, self).__init__()
        self.criterion = nn.KLDivLoss(reduction='sum')
        self.padding_idx = padding_idx
        self.confidence = 1.0 - smoothing
        self.smoothing = smoothing
        self.size = size
        self.true_dist = None

    def forward(self, x, target):
        assert x.size(1) == self.size
        true_dist = x.data.clone()
        true_dist.fill_(self.smoothing / (self.size - 2))
        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)
        true_dist[:, self.padding_idx] = 0
        mask = torch.nonzero(target.data == self.padding_idx)
        if mask.dim() > 0:
            true_dist.index_fill_(0, mask.squeeze(), 0.0)
        self.true_dist = true_dist
        return self.criterion(x, Variable(true_dist, requires_grad=False))

"""## Data Loading

"""

import torchtext
from torchtext import data
from torchtext import datasets
import spacy

# Load spacy tokenizers.
spacy_es = spacy.load('es_core_news_sm')
spacy_en = spacy.load('en_core_web_sm')
spacy_xx = spacy.load('xx_ent_wiki_sm')

def tokenize_ht(text):
    return [tok.text for tok in spacy_xx.tokenizer(text)]

def poor_mans_tokenize_ht(text):
    return text.split()

def tokenize_es(text):
    return [tok.text for tok in spacy_es.tokenizer(text)]

def tokenize_en(text):
    return [tok.text for tok in spacy_en.tokenizer(text)]

"""## Training Code"""

class LossFunction:
    "A simple loss compute and train function."
    def __init__(self, generator, criterion, opt=None):
        self.generator = generator
        self.criterion = criterion
        self.opt = opt

    def __call__(self, x, y, norm):
        x = self.generator(x)
        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),
                              y.contiguous().view(-1)) / norm
        loss.backward()
        if self.opt is not None:
            self.opt.step()
            self.opt.zero_grad()
        return loss.data * norm

class DataIterator(torchtext.legacy.data.Iterator):
    def create_batches(self):
        if self.train:
            def pool(d, random_shuffler):
                for p in torchtext.legacy.data.batch(d, self.batch_size * 100):
                    p_batch = torchtext.legacy.data.batch(
                        sorted(p, key=self.sort_key),
                        self.batch_size, self.batch_size_fn)
                    for b in random_shuffler(list(p_batch)):
                        yield b
            self.batches = pool(self.data(), self.random_shuffler)

        else:
            self.batches = []
            for b in torchtext.legacy.data.batch(self.data(), self.batch_size,
                                          self.batch_size_fn):
                self.batches.append(sorted(b, key=self.sort_key))

import gc
gc.collect()

"""## Translate"""

def greedy_decode(model, src, src_mask, max_len, start_symbol):
    memory = model.encode(src, src_mask)
    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)
    for i in range(max_len-1):
        out = model.decode(memory, src_mask,
                           Variable(ys),
                           Variable(subsequent_mask(ys.size(1))
                                    .type_as(src.data)))
        prob = model.generator(out[:, -1])
        _, next_word = torch.max(prob, dim = 1)
        next_word = next_word.data[0]
        ys = torch.cat([ys,
                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)
    return ys

# Begin execution
if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    parser.add_argument("--src-data-first",
                        default='https://raw.githubusercontent.com/n8rob/corpora/master/ht_total.txt',
                        help="link to source language data for first language pair"
                        )
    parser.add_argument("--tgt-data-first",
                        default='https://raw.githubusercontent.com/n8rob/corpora/master/en_total.txt',
                        help="link to target language data for first language pair"
                        )
    parser.add_argument("--src-data-second",
                        default="",
                        #default='https://raw.githubusercontent.com/n8rob/corpora/master/enfr_fr_fromchurch.txt',
                        help="link to source language data for second language pair; use empty string if only one source language"
                        )
    parser.add_argument("--tgt-data-second",
                        default="",
                        #default='https://raw.githubusercontent.com/n8rob/corpora/master/enfr_en_fromchurch.txt',
                        help="link to target language data for second language pair; use empty string if only one source language"
                        )
    parser.add_argument("--src-first-name",
                        default='haitian',
                        help="name of first source language"
                        )
    parser.add_argument("--tgt-first-name",
                        default='enht_english',
                        help="name of first target language"
                        )
    parser.add_argument("--src-second-name",
                        default="french",
                        help="name of second source language"
                        )
    parser.add_argument("--tgt-second-name",
                        default="enfr_english",
                        help="name of second target language"
                        )
    parser.add_argument("--save-model-path",
                        default="ht2en_transln.pt",
                        help="path to save the trained translation model"
                        )
    parser.add_argument("--download-data",
                        type=int,
                        default=1,
                        help="use 0 to skip downloading the training data (if it is already downloaded)"
                        )
    parser.add_argument("--gpu-num",
                        type=int,
                        default=0,
                        help="which GPU to use"
                        )
    parser.add_argument("--dropout",
                        type=float,
                        default=0.1,
                        help="Dropout rate"
                        )

    args = parser.parse_args()

    args.download_data = bool(args.download_data)

    # Download corpora

    if args.download_data:
        os.system("pip install --upgrade torchtext spacy")
        os.system("python3 -m spacy download en_core_web_sm")
        os.system("python3 -m spacy download es_core_news_sm")
        os.system("python3 -m spacy download xx_ent_wiki_sm")
        os.system('wget  -O ./{} "{}"'.format(args.src_first_name, args.src_data_first))
        os.system('wget -O ./{} "{}"'.format(args.tgt_first_name, args.tgt_data_first))
        if args.src_data_second:
            os.system('wget  -O ./{} "{}"'.format(args.src_second_name, args.src_data_second))
        if args.tgt_data_second:
            os.system('wget -O ./{} "{}"'.format(args.tgt_second_name, args.tgt_data_second))

    # Data Loading code

    BOS_WORD = '<s>'
    EOS_WORD = '</s>'
    BLANK_WORD = "<blank>"
    SRC = torchtext.legacy.data.Field(tokenize=tokenize_ht, pad_token=BLANK_WORD)
    TGT = torchtext.legacy.data.Field(tokenize=tokenize_en, init_token = BOS_WORD,
                     eos_token = EOS_WORD, pad_token=BLANK_WORD)

    print("Loading Dataset")
    # first pair
    src1_text = open('./{}'.format(args.src_first_name),'r')
    src1_lines = list(src1_text)
    tgt1_text = open('./{}'.format(args.tgt_first_name), 'r')
    tgt1_lines = list(tgt1_text)
    # second pair
    if args.src_data_second:
        src2_text = open('./{}'.format(args.src_second_name),'r')
        src2_lines = list(src2_text)
    else:
        src2_lines = []
    if args.tgt_data_second:
        tgt2_text = open('./{}'.format(args.tgt_second_name), 'r')
        tgt2_lines = list(tgt2_text)
    else:
        tgt2_lines = []
    # combine and shuffle
    testing_amount = int(.3*len(src1_lines))
    source_lines = src2_lines + src1_lines
    target_lines = tgt2_lines + tgt1_lines
    assert len(source_lines) == len(target_lines)
    train_limit = len(target_lines) - testing_amount
    # we only want to shuffle the first bit (the training part)
    # because we don't want language2 data in test set
    rand_ord = np.random.permutation(train_limit)
    source_lines = list(np.array(source_lines[:train_limit])[rand_ord]) + source_lines[train_limit:]
    target_lines = list(np.array(target_lines[:train_limit])[rand_ord]) + target_lines[train_limit:]
    #db.set_trace()

    fields = (["src", SRC], ["trg", TGT])
    print("LENS", len(target_lines), len(source_lines))
    examples = [torchtext.legacy.data.Example.fromlist((source_lines[i], target_lines[i]), fields ) for i in range(len(target_lines))]
    train_examples, val_examples = examples[:train_limit], examples[train_limit:]

    MAX_LEN = 200
    train, val = torchtext.legacy.data.Dataset(train_examples, fields=fields, filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and
            len(vars(x)['trg']) <= MAX_LEN), torchtext.legacy.data.Dataset(val_examples, fields=fields, filter_pred=
            lambda x: len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN)

    MIN_FREQ = 1
    SRC.build_vocab(train.src, min_freq=MIN_FREQ)
    TGT.build_vocab(train.trg, min_freq=MIN_FREQ)

    # Training code
    
    def rebatch(pad_idx, batch):
        "Fix order in torchtext to match ours"
        src, trg = batch.src.transpose(0, 1).to(torch.device("cuda:{}".format(args.gpu_num))), batch.trg.transpose(0, 1).to(torch.device("cuda:{}".format(args.gpu_num)))
        return Batch(src, trg, pad_idx)

    def run_epoch(data_iter, model, loss_compute):
        "Standard Training and Logging Function"
        start = time.time()
        total_tokens = 0
        total_loss = 0
        tokens = 0
        for i, batch in enumerate(data_iter):
            out = model.forward(batch.src, batch.trg,
                                batch.src_mask, batch.trg_mask)
            loss = loss_compute(out, batch.trg_y, batch.ntokens)
            total_loss += loss
            total_tokens += batch.ntokens
            tokens += batch.ntokens
            if i % 50 == 1:
                elapsed = time.time() - start
                print("Epoch Step: %d Loss: %f Tokens per Sec: %f" %
                        (i, loss / batch.ntokens, tokens / elapsed))
                start = time.time()
                tokens = 0
        return total_loss / total_tokens

    pad_idx = TGT.vocab.stoi["<blank>"]
    model = TransformerModel(len(SRC.vocab), len(TGT.vocab), N=2, dropout=args.dropout).to(torch.device("cuda:{}".format(args.gpu_num)))
    n_epochs = 3
    device = torch.device('cuda')

    def scope():
        criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=0.1)
        criterion.to(torch.device("cuda:{}".format(args.gpu_num)))
        BATCH_SIZE = 1000
        train_iter = DataIterator(train, batch_size=BATCH_SIZE, device=device,
                                repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),
                                batch_size_fn=batch_size_fn, train=True)
        valid_iter = DataIterator(val, batch_size=BATCH_SIZE, device=device,
                                repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),
                                batch_size_fn=batch_size_fn, train=False)

        model_opt = torch.optim.Adam(model.parameters(), lr=5e-4)
        for epoch in range(n_epochs):
            model.train()
            run_epoch((rebatch(pad_idx, b) for b in train_iter),
                      model,
                      LossFunction(model.generator, criterion, model_opt))
            model.eval()

    scope()

    # Validation code

    BATCH_SIZE = 1000
    n_train_iters = len(train) / BATCH_SIZE
    valid_iter = DataIterator(val, batch_size=BATCH_SIZE, device=device,
                            repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),
                            batch_size_fn=batch_size_fn, train=False)
    refs = []
    preds = []
    for i, batch in enumerate(valid_iter):
        src = batch.src.transpose(0, 1)[:1].to(torch.device("cuda:{}".format(args.gpu_num)))
        src_mask = (src != SRC.vocab.stoi["<blank>"]).unsqueeze(-2).to(torch.device("cuda:{}".format(args.gpu_num)))
        out = greedy_decode(model, src, src_mask,
                            max_len=60, start_symbol=TGT.vocab.stoi["<s>"])
        #print("source:", end="\t")
        for i in range(0, src.size(1)):
            sym = SRC.vocab.itos[src[0, i]]
            if sym == "</s>": break
            #print(sym, end =" ")
        #print()
        trans = []
        #print("Translation:", end="\t")
        for i in range(1, out.size(1)):
            sym = TGT.vocab.itos[out[0, i]]
            if sym == "</s>": break
            trans.append(sym)
            #print(sym, end =" ")
        #print()
        #print("Target:\t", end="\t")
        sent = []
        for i in range(1, batch.trg.size(0)):
            sym = TGT.vocab.itos[batch.trg.data[i, 0]]
            if sym == "</s>": break
            sent.append(sym)
            #print(sym, end =" ")
        #print()
        #print()

        # target
        sent = ' '.join(sent).strip().split()
        #print("1", sent)
        sent = md.detokenize(sent)
        #print("2", sent)
        refs.append(sent)

        # prediction
        trans = ' '.join(trans).strip().split()
        #print("1", trans)
        trans = md.detokenize(trans)
        #print("2", trans)
        preds.append(trans)

        if i > 1000 and i<1100:
            break

    # Calculate and print the BLEU score
    refs = [refs]
    print("lens", len(preds), len(refs))
    print(preds, refs)
    bleu = sacrebleu.corpus_bleu(preds, refs)
    print(bleu.score)

    """<div id="disqus_thread"></div>
    <script>
        /**
         *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
         *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
         */
        /*
        var disqus_config = function () {
            this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
            this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
        };
        */
        (function() {  // REQUIRED CONFIGURATION VARIABLE: EDIT THE SHORTNAME BELOW
            var d = document, s = d.createElement('script');
    
            s.src = 'https://EXAMPLE.disqus.com/embed.js';  // IMPORTANT: Replace EXAMPLE with your forum shortname!
    
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
    
    ##Manual testing
    """

    # Save model and vocab information

    torch.save(model, args.save_model_path)
    # And now save the vocab info 
    vocab_info = {
            "SRC.vocab.stoi": SRC.vocab.stoi,
            "SRC.vocab.itos": SRC.vocab.itos,
            "TGT.vocab.stoi": TGT.vocab.stoi,
            "TGT.vocab.itos": TGT.vocab.itos
            }
    save_path_base = args.save_model_path.split(".")[0]
    save_vocab_path = save_path_base + "_VOCAB.pkl"
    with open(save_vocab_path, 'wb') as f:
        pkl.dump(vocab_info, f)

    USER_TESTING = False

    if USER_TESTING:
        while True:
            ht_text = input("Enter a sentence in Haitian / Tape yon fraz an krey√≤l\n(Type 'stop' to stop):\t")

            if ht_text.lower() == 'stop':
                break

            src = torch.Tensor([0] + [SRC.vocab.stoi[word] for word in ht_text.split()] + [0]).unsqueeze(0).long().to(torch.device("cuda:{}".format(args.gpu_num)))
            src_mask = (src != SRC.vocab.stoi["<blank>"]).unsqueeze(-2).to(torch.device("cuda:{}".format(args.gpu_num)))
            out = greedy_decode(model, src, src_mask,
                        max_len=60, start_symbol=TGT.vocab.stoi["<s>"])
            # print("haitian:", end="\t")
            # for i in range(0, src.size(1)):
            #     sym = SRC.vocab.itos[src[0, i]]
            #     if sym == "</s>": break
            #     print(sym, end =" ")
            # print()
            print("Translation:", end="\t")
            for i in range(1, out.size(1)):
                sym = TGT.vocab.itos[out[0, i]]
                if sym == "</s>": break
                print(sym, end =" ")
            print()
            print()

"""
Example command line run
python3 n8_trans.py --download-data 0 --src-data-first https://raw.githubusercontent.com/n8rob/corpora/master/ht_total.txt --tgt-data-first https://raw.githubusercontent.com/n8rob/corpora/master/en_total.txt --src-data-second https://raw.githubusercontent.com/n8rob/corpora/master/enfr_fr_fromchurch.txt --tgt-data-second https://raw.githubusercontent.com/n8rob/corpora/master/enfr_en_fromchurch.txt --src-first-name haitian --tgt-first-name enht_english --src-second-name french --tgt-second-name enfr_english --save-model-path ht2en_transln.pt --gpu-num 0
"""
